---
title: "Simple Regression Analysis"
author: "Andrea Widjaja"
date: "10/07/16"
output:
  pdf_document:
    fig_caption: yes
  html_document: default
header-includes: \usepackage{float}
---

# Stat 159 Homework 2

# Abstract  

This paper consists of the reproduced results from _Section 3.1 Simple Linear Regression_ from the book **An Introduction to Statistical Learning** _by Gareth James, Daniel Witten, Trevor Hastie, and Robert Tibshirani._ This paper will discuss topics of simple linear regression such as estimating the coefficients, assessing the accuracy of the coefficient estimates, and assessing the accuracy of the model. (least square approach)

# Introduction  
Suppose we are given information regarding the sales progress of a particular product (in thousands of units) and advertising budgets (in thousands of dollar) for three medias. As statistical consultants, we are given a task to develop a marketing plan that will result in high product sales with minimum advertising budget possible. Before tackling this problem, there are several factors that we must consider. First of all, we should determine whether there is a relationship between advertising budget and sales. If there is no relationship, then there is no point in spending money on advertising. If there is, however, a relationship, then we should know the strength of this relationship. Meaning that, if we are given the advertising budget, are we able to predict sales with a high level of accuracy?

This is when Simple Linear Regression steps in. Linear regression can be utilized for predicting a quantitative estimate to provied a data-based solution.

# Data  
The data set we are given is *Advertising.csv*, downloaded from `http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv.` This data set contains the sales(in thousands of units) of a product across 200 different markets and the amount spent(in thousands of dollars) in advertising for the product in each market. The advertising data contains budgets for three different medias: TV, radio and newspaper. It has 200 records and 5 variables(index, TV, Radio, Newspaper, Sales). Each record contains the information of advertising budget for TV, Radio, and Newspaper, including the sales progress for each market.  

For this paper, we will focus on only the TV media and analyze its relationship to sales.  


# Methodology  
##Linear Regression
Suppose that we want to estimate a relationship between two variables $X$ and $Y$. We assume that there is a linear relationship between the Variable $X$ and Variable $Y$. For example, if we presume that a change in $X$ has an effect on $Y$, then we can say that $X$ is the independent variable, whereas $Y$ is the dependent variable. This relationship can be represented as a simple lienar model.  

Regress $Y$ on $X$:
$$ Y = \beta_0 + \beta_1X $$  
This, however, do not take into account for the error term/residual. The dependent variable $Y$, may not be perfectly predicted by the variable $X$. The error term/residual represents the difference between the results obtained by the model and real-world applications. 

If we take into account for error, the linear regression will be:
$$ Y = \beta_0 + \beta_1X + \epsilon $$

For our case, we need to analyze the relationship between TV advertising and Sales. The first step we need to do is come up with a similar linear model. $X$, for example, can represent TV, whereas, $Y$ can represent Sales.  

Regress Sales on TV advertising, taking into account for residual:
$$ Sales = \beta_0 + \beta_1 * TV + \epsilon $$  

###Estimating the Coefficient
From the linear models above, $\beta_0$ and $\beta_1$ are constants. $\beta_1$ determines the slope, and $\beta_0$ determines the intercept. They are also known as coefficients or parameters.  

Unfortunately, in practice, these betas are unknown. We must use data to estimate them. Let's say that we observed n pairs of data: $$ (TV_1, Sales_1), (TV_2, Sales_2),..., (TV_n, Sales_n) $$ We have to find the coefficient intercept estimate $\hat{\beta_0}$ and slope estimate $\hat{\beta_1}$ such that the linear regression fits the data well. To achieve the closest line possible to the data points, we can use the least square criterion.  

Let the predicition for Y based on ith value of X be:
$$ \hat{y} = \hat{\beta_0} + \hat{\beta_1}* x_i $$
and ith residual (estimate of the error term) be:
$$ \epsilon_i = y_i - \hat{y_i} $$  

Residual Sum of Squares (RSS):
$$ RSS = \sum_{i=1}^n(\epsilon_i)^2 = \epsilon_1^2 + \epsilon_2^2 + \epsilon_3^2 + ... + \epsilon_n^2 $$
or simply,
$$ RSS = \sum_{i=1}^n (y_i - \beta_0 - \beta_1x_i)^2 = (y_1 - \hat{\beta_0} - \hat{\beta_1}x_1)^2 + (y_2 - \hat{\beta_0} - \hat{\beta_1}x_2)^2 + ... + (y_n - \hat{\beta_0} - \hat{\beta_1}x_n)^2$$

For the least square approach, solving for $(\hat{\beta_0},\hat{\beta_1})$ minimizes the RSS.

$$\hat{\beta_1} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}$$
$$\hat{\beta_0} = \bar{y} - \hat{\beta_1}*\bar{x}$$

Where $\bar{y}=\frac{\sum_{i=1}^n(y_i)}{n}$ and $\bar{x}=\frac{\sum_{i=1}^n(x_i)}{n}$ are sample means.  

We get: $\hat{\beta_0} = 7.03$ and $\hat{\beta_1} = 0.0475$

This solves for the least squares coefficient estimates for the simple linear regression.

### Asessing the Accuracy of the Coefficient Estimates  
Since standard errors can measure accuracy, standard errors are used to perform *hypothesis tests* on the coefficients.
We are going to test the *null hypothesis* versus the *alternative hypotehsis*.  

Null hypothesis :
  $$ H_0: There Is No Relationship Between X and Y $$
  $$ H_0: \beta_1 = 0 $$

versus  

Alternative hypothesis:
  $$ H_a: There Is Some Relationship Between X and Y $$
  $$ H_a: \beta_1 \neq 0 $$

When testing the null hypothesis, we need to know how far $\hat{\beta_1}$ is from 0. This depends on the standard error of $\hat{\beta_1}$. If the $SE(\hat{\beta_1})$ is small, then we can conclude that $\beta_1 \neq 0$ and that there is a relationship between X and Y in the linear model $$Y = \beta_0 + \beta_1X + \epsilon $$.  

On the contrary, if $SE(\hat{\beta_1})$ is large, then $\beta_1$ is large, such that we have to reject the null hypothesis.


t-statistic:
$$ t = \hat{\beta_1} - 0 / SE(\beta_1) $$
measures the number of standard deviations that $\hat{\beta_1}$ is away from 0.

###Assessing the Accuracy of the Model 
If we reject the null hypothesis, and go for the alternative hypothesis instead, we should measure the extent to which the model fits the data. The quality of a linear regression fit is assessed using two related quantities: *Residual Standard Error* and $R^2 Statistic$.

####Residual Standard Error(RSE) 
The RSE is an estimate of the standard deviation of $\epsilon$. It is the average amount that the response will deviate from the true regression line. Additionally, in simple words, it is a measure of the lack of fit of the linear model to the data.
$$ RSE = \sqrt {( \frac{1}{n-2}) * RSS} $$
$$ RSE = \sqrt {( \frac{1}{n-2}) * (\sum_{i=1}^n (y_i - \hat{y}_i)^2)}$$

When the RSE is small, then we can assume that for $i = 1,...,n$, $\hat{y}_i=y_i$.
This means that we can conclude that the model fits the data very well.
On the contrary, if $\hat{y}_i$ is very far from $y_i$, for one or more of the observations, then the RSE will be large, indicating that the model does not fit the data very well.

###R^2 Statistic
The $R^2 Statistic$ provides an alternative measure of fit to the RSE.

The formula of $R^2$ is given by:
$$ R^2 = \frac{(TSS - RSS)}{TSS} \ = 1 - \frac{RSS}{TSS} $$
where,
$$ TSS = {\sum (y_i - \hat{y}_i)^2 } $$ is the total sum of squares.





It is the least squares fit for the regression of sales onto TV. Fit is found by minimiizng the sum of squared erros.

In other words, the line represents a simple model that can be used to predict *Sales* using the TV medium.



# Results  

```{r echo=FALSE, fig.width=6, fig.height=4, fig.pos='H', fig.align='center', fig.cap="scatterplot"}
library(png)
library(grid)
img <- readPNG("../images/scatterplot-tv-sales.png")
grid.raster(img)
```


We compute the regression coefficients

```{r results='asis', message=FALSE, echo=FALSE, warning=FALSE}
load('../data/regression.RData')
library(xtable)
regression_table <- xtable(RegressionSummary$coefficients, digits=4,
                           caption='Information about Regression Coefficients')
print(regression_table, type = "latex", comment=FALSE, caption.placement='top')
```

More information about the least square model is given in the table below:

```{r results='asis', message=FALSE, echo=FALSE}
x = data.frame(
  label = c("RSE", "R2", "F-statistic"),
  stats = c(RegressionSummary$sigma, RegressionSummary$r.squared, RegressionSummary$fstatistic[1]))
regression_table2 <- xtable(x, digits=4,
                           caption='Regression Quality Indices')
print(regression_table2, type = "latex", comment=FALSE, caption.placement='top')
```




# Conclusions